{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FDS11 Daniel Geffers.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bZEDQgDiY7M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "documents = []\n",
        "documents.append(\"Romeo and Juliet.\")\n",
        "documents.append(\"Juliet: O happy dagger!\")\n",
        "documents.append(\"Romeo died by dagger.\")\n",
        "documents.append(\"\\\"Live freee or die\\\", that's the New-Hampshire's motto.\")\n",
        "documents.append(\"Did you know, New-Hampshire is in New-England.\")\n",
        "\n",
        "\n",
        "def get_with_k(A, k):\n",
        "    u, s, vh = np.linalg.svd(A)\n",
        "    #print(\"calculated\", len(s), \"SVs\", u.shape, vh.shape)\n",
        "    S = np.zeros((len(s), len(s)))\n",
        "    np.fill_diagonal(S, s)\n",
        "    return u[:,:k], S[:k,:k], vh[:k, :]\n",
        "\n",
        "def run_query(A, k, query_words, scaling=True, euclidean=False, look_for_documents=True):\n",
        "    q = make_query_from_words(A, k, query_words)\n",
        "    u, S, vh = get_with_k(A, k)\n",
        "    # the documents are given by s*vh\n",
        "    if scaling:\n",
        "        word_matrices = np.matmul(u, S)\n",
        "        document_matrices = np.matmul(S, vh).T\n",
        "    else:\n",
        "        word_matrices = u\n",
        "        document_matrices = vh.T\n",
        "    results_list = []\n",
        "    if look_for_documents:\n",
        "        list_to_look_in = document_matrices\n",
        "    else:\n",
        "        list_to_look_in = word_matrices\n",
        "    for i in range(len(list_to_look_in)):\n",
        "        word_or_doc = list_to_look_in[i]\n",
        "        if euclidean:\n",
        "            temp = dist = np.linalg.norm(word_or_doc-q)\n",
        "        else:\n",
        "            temp = np.matmul(word_or_doc, q)\n",
        "            temp = temp/(np.linalg.norm(word_or_doc)*np.linalg.norm(q))\n",
        "        results_list.append(temp)\n",
        "    return results_list\n",
        "\n",
        "def make_query_from_words(matrix, k, words):\n",
        "    u, S, vh = get_with_k(matrix, k)\n",
        "    # the documents are given by s*vh\n",
        "    word_matrices = np.matmul(u, S)\n",
        "    sum = np.zeros((k))\n",
        "    counter = 0\n",
        "    for i in range(len(words)):\n",
        "        if words[i]:\n",
        "            sum += word_matrices[i]\n",
        "            counter += 1\n",
        "    return sum/counter\n",
        "\n",
        "A_local = [\n",
        "     [1,0,1,0,0],\n",
        "     [1,1,0,0,0],\n",
        "     [0,1,0,0,0],\n",
        "     [0,1,1,0,0],\n",
        "     [0,0,0,1,0],\n",
        "     [0,0,1,1,0],\n",
        "     [0,0,0,1,0],\n",
        "     [0,0,0,1,1]\n",
        "]\n",
        "    "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoFV0u-0jwds",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "outputId": "7ab01a5d-be0f-4a23-c3bf-aa7b8af1b1d3"
      },
      "source": [
        "# i)\n",
        "# reconstruct old example\n",
        "k_local = 2\n",
        "u, S, vh = get_with_k(A_local, k_local)\n",
        "print(u)\n",
        "print(S)\n",
        "print(vh)\n",
        "word_matrices = np.matmul(u, S)\n",
        "print(\"word_matrices\", word_matrices)\n",
        "document_matrices = np.matmul(S, vh).T\n",
        "print(\"document_matrices\", document_matrices)\n",
        "\n",
        "query_words = [0,0,0,1,0,1,0,0]\n",
        "# q = make_query_from_words(A, k, query_words)\n",
        "print(\"Using query q\", query_words)\n",
        "results = run_query(A_local, k_local, query_words)\n",
        "print(\"the resulting distances to the documents from the query correspond to:\\n\", results)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.39615277  0.28005737]\n",
            " [-0.31426806  0.44953214]\n",
            " [-0.17823952  0.26899154]\n",
            " [-0.43836375  0.36850831]\n",
            " [-0.26388058 -0.34592143]\n",
            " [-0.52400482 -0.24640466]\n",
            " [-0.26388058 -0.34592143]\n",
            " [-0.32637322 -0.45966878]]\n",
            "[[2.28529793 0.        ]\n",
            " [0.         2.01025824]]\n",
            "[[-0.31086574 -0.40733041 -0.59446137 -0.60304575 -0.1428143 ]\n",
            " [ 0.36293322  0.54074246  0.20005441 -0.6953914  -0.22866156]]\n",
            "word_matrices [[-0.90532712  0.56298763]\n",
            " [-0.71819615  0.90367568]\n",
            " [-0.40733041  0.54074246]\n",
            " [-1.00179178  0.74079687]\n",
            " [-0.60304575 -0.6953914 ]\n",
            " [-1.19750713 -0.49533699]\n",
            " [-0.60304575 -0.6953914 ]\n",
            " [-0.74586005 -0.92405295]]\n",
            "document_matrices [[-0.71042084  0.7295895 ]\n",
            " [-0.93087134  1.08703198]\n",
            " [-1.35852135  0.40216102]\n",
            " [-1.37813921 -1.39791629]\n",
            " [-0.32637322 -0.45966878]]\n",
            "Using query q [0, 0, 0, 1, 0, 1, 0, 0]\n",
            "the resulting distances to the documents from the query correspond to:\n",
            " [0.7727964887537564, 0.7306768205359736, 0.9844359912676066, 0.6187306127613201, 0.4849183185073809]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xeYd3uVr-l2",
        "colab_type": "text"
      },
      "source": [
        "Just as in the example I find that the 3. Text \"Romeo died by dagger.\" best fits the query. Followed by the first 2 texts with similar values. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XJBUoKTnt3U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "7048ed55-cb23-4db7-9570-04ab3ec1fac2"
      },
      "source": [
        "# ii) a)\n",
        "# modefy k\n",
        "\n",
        "for k_a in range(2, 6):\n",
        "    results = run_query(A_local, k_a, query_words)\n",
        "    print(\"going with k\", k_a)\n",
        "    print(results)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "going with k 2\n",
            "[0.7727964887537564, 0.7306768205359736, 0.9844359912676066, 0.6187306127613201, 0.4849183185073809]\n",
            "going with k 3\n",
            "[0.7790887632803176, 0.4962370290822853, 0.9206238960480264, 0.5742675138280299, 0.31817696222835556]\n",
            "going with k 4\n",
            "[0.2598311922704977, 0.5214034484031386, 0.9153977388771566, 0.5173749167851798, 0.14161281019977673]\n",
            "going with k 5\n",
            "[0.25757392536334167, 0.5208900313918557, 0.9129757621476984, 0.5099090479804408, 0.11282240858457236]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6oXTEDusc-s",
        "colab_type": "text"
      },
      "source": [
        "Larger k seem to confuse the system. At k >= 4 there seem only to stay the literal readings of \"die\" or \"dagger\" as good fits. I expect this behavior due to be due to k describing the number of concepts understood. If k is small, then similar words as Romeo or Juliet are considered almost no different than die and dagger. Once more dimentions are allowed the difference between them becomes more pronounced."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6Px5giY0IHU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "db335eeb-ec37-456b-d8c0-9dd8b8a4bc4f"
      },
      "source": [
        "# ii) b)\n",
        "# omitting the scaling step\n",
        "print(\"with scaling   \", run_query(A_local, k_local, query_words, scaling=True))\n",
        "print(\"without scaling\", run_query(A_local, k_local, query_words, scaling=False))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "with scaling    [0.7727964887537564, 0.7306768205359736, 0.9844359912676066, 0.6187306127613201, 0.4849183185073809]\n",
            "without scaling [0.7307535536972931, 0.6865585886766306, 0.9773001857547076, 0.5673208751646639, 0.43238767985684207]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qQq4Cbo6ZXV",
        "colab_type": "text"
      },
      "source": [
        "Both in this example and in the wikipedia analysis below scaling seems to have only a small impact"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7rilKZO2K2g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "772d42ba-7d52-4034-9c05-8b150a00f4eb"
      },
      "source": [
        "# ii) c)\n",
        "# omitting words that only occur in a single document\n",
        "A_no_single_apperances = [\n",
        "     [1,0,1,0,0],\n",
        "     [1,1,0,0,0], # third removed\n",
        "     [0,1,1,0,0],# fifth removed\n",
        "     [0,0,1,1,0],# seventh removed\n",
        "     [0,0,0,1,1]\n",
        "]\n",
        "A_additional_single_words = [\n",
        "     [1,0,1,0,0],\n",
        "     [1,1,0,0,0],\n",
        "     [0,1,0,0,0],\n",
        "     [0,1,1,0,0],\n",
        "     [0,0,0,1,0],\n",
        "     [0,0,1,1,0],\n",
        "     [0,0,0,1,0],\n",
        "     [0,0,0,1,1],\n",
        "     [0,0,0,1,0],\n",
        "     [0,0,1,0,0],\n",
        "     [1,0,0,0,0]\n",
        "]\n",
        "A_additional_words = [\n",
        "     [1,0,1,0,0],\n",
        "     [1,1,0,0,0],\n",
        "     [0,1,0,0,0],\n",
        "     [0,1,1,0,0],\n",
        "     [0,0,0,1,0],\n",
        "     [0,0,1,1,0],\n",
        "     [0,0,0,1,0],\n",
        "     [0,0,0,1,1],\n",
        "     [1,0,1,1,1],# this is the word \".\"\n",
        "     [0,0,0,1,1], #this is the \",\"\n",
        "     [1,1,0,0,0] # this does not correspond to something\n",
        "]\n",
        "print(\"original            \", run_query(A_local, k_local, query_words))\n",
        "print(\"no_single_apperances\", run_query(A_no_single_apperances, k_local, [0,0,1,1,0]))# note that the query words needs to be changed\n",
        "print(\"A_additional_single_words\", run_query(A_additional_single_words, k_local, query_words))\n",
        "print(\"A_additional_words\", run_query(A_additional_words, k_local, query_words))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original             [0.7727964887537564, 0.7306768205359736, 0.9844359912676066, 0.6187306127613201, 0.4849183185073809]\n",
            "no_single_apperances [0.6722877510014041, 0.6722877510014048, 0.9902753662967518, 0.6817842761697039, 0.48229661925975065]\n",
            "A_additional_single_words [0.7436413533210913, 0.7436413533210912, 0.9696659529652373, 0.5975427749192898, 0.4692685032998632]\n",
            "A_additional_words [0.8992251356798536, 0.6518396638922946, 0.9997255585156124, 0.6787131631456086, 0.7141359574633465]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4O3Y_55Z5n--",
        "colab_type": "text"
      },
      "source": [
        "Adding or removing words, changes the exact situation. However the similarities between texts and keywords, stay approximately the same, as long as no new word is linking texts. This holds true for different k also. (Note that upon removing words, the query words vector must be changed to fit!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3POYbKy4Qwz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "ecae9570-bb60-4b74-8922-d33f5c8de31d"
      },
      "source": [
        "# ii) d)\n",
        "# use eucleadian metric\n",
        "print(\"with cosine    metric\", run_query(A_local, k_local, query_words, euclidean=False))\n",
        "print(\"with euclidean metric\", run_query(A_local, k_local, query_words, euclidean=True))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "with cosine    metric [0.7727964887537564, 0.7306768205359736, 0.9844359912676066, 0.6187306127613201, 0.4849183185073809]\n",
            "with euclidean metric [0.7209559279938733, 0.9789609241466175, 0.38091519694220033, 1.5459370903491112, 0.9680621858502625]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onBRpWzcyItH",
        "colab_type": "text"
      },
      "source": [
        "The euclidean metric involes direct distances, that have serval downsides. The euclidean distance may reach arbitrairily large numbers (this may be fixed by norming it), is less percisely resolved in the relevant range and measures what one may intuitivly describe as \"absolute\" distances. Since here we are interested in differences, not absolute distances, this will signifikantly confuse.\n",
        "Angular distances on the other hand are well normed to be below 1 and take into account/compensate for both the length of the texts and the query."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cQ9uEbP8HsR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "outputId": "f5b7e0f4-5fac-4801-f4ba-3f3d972e9fc5"
      },
      "source": [
        "# iii) larger dataset\n",
        "\n",
        "wiki = []\n",
        "wiki.append(\"The Federal City of Bonn is a city on the banks of the Rhine in the German state of North Rhine-Westphalia, with a population of over 300,000. About 24 km south-southeast of Cologne, Bonn is in the southernmost part of the Rhine-Ruhr region, Germany's largest metropolitan area, with over 11 million inhabitants. It is famously known as the birthplace of Ludwig van Beethoven in 1770. Beethoven spent his childhood and teenage years in Bonn.\")\n",
        "wiki.append(\"Aachen, also known as Bad Aachen (\\\"Aachen Spa\\\"), and in French and traditional English as Aix-la-Chapelle, is a spa and border city in North Rhine-Westphalia, Germany. Aachen developed from a Roman settlement and spa, subsequently becoming the preferred medieval Imperial residence of Charlemagne, and, from 936 to 1531, the place where 31 Holy Roman Emperors were crowned Kings of the Germans.\")\n",
        "wiki.append(\"Duisburg is a city of about 500,000 inhabitants in Germany’s Rhineland, at the confluence of the Rhine and the Ruhr. In medieval times, it was a member of the powerful Hanseatic League, and later became a major centre of iron, steel, and chemicals. For this reason, it was heavily bombed in World War II. Today it boasts the world's largest inland port, with 21 docks and 40 kilometres of wharf. The city supports a large Turkish community.\")\n",
        "wiki.append(\"Münster is an independent city in North Rhine-Westphalia, Germany. It is in the northern part of the state and is considered to be the cultural centre of the Westphalia region. It is also capital of the local government region Münsterland. Münster was the location of the Anabaptist rebellion during the Protestant Reformation and the site of the signing of the Treaty of Westphalia ending the Thirty Years' War in 1648. Today it is known as the bicycle capital of Germany.\")\n",
        "wiki.append(\"Dortmund is, with a population of 586,600 (2017), the third-largest city of Germany's most populous federal state of North Rhine-Westphalia after Cologne and Düsseldorf, and Germany's eighth-largest city. It is the largest city (by area and population) of the Ruhr, Germany's largest urban area with some 5.1 million (2011) inhabitants, as well as the largest city of Westphalia. On the Emscher and Ruhr rivers (tributaries of the Rhine), it lies in the Rhine-Ruhr Metropolitan Region and is considered the administrative, commercial, and cultural centre of the eastern Ruhr.\")\n",
        "wiki.append(\"Ludwig van Beethoven was a German composer and pianist. A crucial figure in the transition between the classical and romantic eras in classical music, he remains one of the most recognized and influential musicians of this period, and is considered to be one of the greatest composers of all time. \")\n",
        "wiki.append(\"Franz Joseph Haydn was an Austrian composer of the Classical period. He was instrumental in the development of chamber music such as the piano trio. His contributions to musical form have earned him the epithets \\\"Father of the Symphony\\\" and \\\"Father of the String Quartet\\\". He was a friend and mentor of Mozart, a tutor of Beethoven, and the older brother of composer Michael Haydn.\")\n",
        "wiki.append(\"Haribo is a German confectionery company founded in 1920 by Johannes \\\"Hans\\\" Riegel, Sr. It began in Kessenich, Bonn, North Rhine-Westphalia; the name is an acronym formed from Hans Riegel, Bonn. The company created the first gummy candy in 1922 in the form of little gummy bears called Gummibärchen. The current headquarters in is Grafschaft, Rhineland-Palatinate, Germany.\")\n",
        "\n",
        "wiki.append(\"Classical music is art music produced or rooted in the traditions of Western culture, including both liturgical (religious) and secular music. While a more precise term is also used to refer to the period from 1750 to 1820 (the Classical period), this article is about the broad span of time from before the 6th century AD to the present day, which includes the Classical period and various other periods.[1] The central norms of this tradition became codified between 1550 and 1900, which is known as the common-practice period.\")\n",
        "wiki.append(\"This is a very short fake article about rock music. Rock, metal and Münster.\")\n",
        "\n",
        "def text_to_list(text):\n",
        "    list_to_return = []\n",
        "    text = clean_word(text)# we only deal with lowercase words, to have words at the start of a sentence and at the end be equal\n",
        "    for word in text.split(): # split at all types of whitespace\n",
        "        # word = clean_word(word)\n",
        "        list_to_return.append(word)\n",
        "    # alternativly split at various fixed lengths\n",
        "    # this could also be done not on word level but on the full text with much longer length.\n",
        "    #    for length in range(3,10):\n",
        "    #        for char_index in range(len(word)-length):\n",
        "    #            list_to_return.append( word[char_index:char_index+length] )\n",
        "    return list_to_return\n",
        "\n",
        "def clean_word(word):\n",
        "    word = word.lower()\n",
        "    word = word.replace(\",\", \"\") # remove some symbols\n",
        "    word = word.replace(\".\", \"\")\n",
        "    word = word.replace(\";\", \"\")\n",
        "    word = word.replace(\"!\", \"\")\n",
        "    word = word.replace(\"?\", \"\")\n",
        "    word = word.replace(\":\", \"\")\n",
        "    word = word.replace(\"\\\"\", \"\")\n",
        "    word = word.replace(\"(\", \"\")\n",
        "    word = word.replace(\")\", \"\")\n",
        "    return word\n",
        "\n",
        "list_of_all_words = []\n",
        "wiki_matrix = []\n",
        "for wiki_index in range(len(wiki)):\n",
        "    text = wiki[wiki_index]\n",
        "    for word in text_to_list(text):\n",
        "        if word in [\"in\", \"and\", \"or\", \"the\", \"a\", \"is\", \"of\", \"with\", \"as\", \"at\", \"this\", \"about\"]:\n",
        "            continue\n",
        "        if word not in list_of_all_words:\n",
        "            list_of_all_words.append(word)\n",
        "        \n",
        "        # add it to the matrix\n",
        "        word_index = list_of_all_words.index(word)\n",
        "        if len(wiki_matrix) == word_index:\n",
        "            wiki_matrix.append(np.zeros(len(wiki)))\n",
        "        wiki_matrix[word_index][wiki_index] += 1\n",
        "        # this gives A\n",
        "\n",
        "def print_data():\n",
        "    for word_index in range(len(wiki_matrix)):\n",
        "        to_print = str(word_index)+\" \"+list_of_all_words[word_index]\n",
        "        for wiki_index in range(len(wiki)):\n",
        "            to_print += \" \"+str(wiki_matrix[word_index][wiki_index])\n",
        "        print(to_print)\n",
        "\n",
        "def get_query_vector(search_string):\n",
        "    search_string = clean_word(search_string)\n",
        "    vector = np.zeros(len(list_of_all_words))\n",
        "    for word_index in range(len(list_of_all_words)):\n",
        "        word = list_of_all_words[word_index]\n",
        "        if word in text_to_list(search_string): # todo repeated words\n",
        "            vector[word_index] += 1\n",
        "    return vector\n",
        "\n",
        "def wiki_search(search_string, amount_of_results, k=7):\n",
        "    vector = get_query_vector(search_string)\n",
        "    # print(vector)\n",
        "    word_similarity_vector = np.array( run_query(wiki_matrix, k, vector, look_for_documents=False) )\n",
        "    #print(\"word_similarity_vector\", word_similarity_vector)\n",
        "    doc_similarity_vector = np.array( run_query(wiki_matrix, k, vector, look_for_documents=True, scaling=True) )\n",
        "    #print(\"doc_similarity_vector\", doc_similarity_vector)\n",
        "\n",
        "    #print(\"Top similar words to the query >\"+search_string+\"< are:\")\n",
        "    #for word_index in (-word_similarity_vector).argsort()[:amount_of_results]:\n",
        "    #    print(list_of_all_words[word_index])\n",
        "\n",
        "    print(\"Top similar texts to the query >\"+search_string+\"< are:\")\n",
        "    for wiki_index in (-doc_similarity_vector).argsort()[:amount_of_results]:\n",
        "        print(wiki_index, \":\", wiki[wiki_index])\n",
        "    return\n",
        "\n",
        "# print_data()\n",
        "wiki_search(\"classical music\", 5, k=5)\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top similar texts to the query >classical music< are:\n",
            "8 : Classical music is art music produced or rooted in the traditions of Western culture, including both liturgical (religious) and secular music. While a more precise term is also used to refer to the period from 1750 to 1820 (the Classical period), this article is about the broad span of time from before the 6th century AD to the present day, which includes the Classical period and various other periods.[1] The central norms of this tradition became codified between 1550 and 1900, which is known as the common-practice period.\n",
            "9 : This is a very short fake article about rock music. Rock, metal and Münster.\n",
            "5 : Ludwig van Beethoven was a German composer and pianist. A crucial figure in the transition between the classical and romantic eras in classical music, he remains one of the most recognized and influential musicians of this period, and is considered to be one of the greatest composers of all time. \n",
            "6 : Franz Joseph Haydn was an Austrian composer of the Classical period. He was instrumental in the development of chamber music such as the piano trio. His contributions to musical form have earned him the epithets \"Father of the Symphony\" and \"Father of the String Quartet\". He was a friend and mentor of Mozart, a tutor of Beethoven, and the older brother of composer Michael Haydn.\n",
            "3 : Münster is an independent city in North Rhine-Westphalia, Germany. It is in the northern part of the state and is considered to be the cultural centre of the Westphalia region. It is also capital of the local government region Münsterland. Münster was the location of the Anabaptist rebellion during the Protestant Reformation and the site of the signing of the Treaty of Westphalia ending the Thirty Years' War in 1648. Today it is known as the bicycle capital of Germany.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBsvvWsLeZfo",
        "colab_type": "text"
      },
      "source": [
        "Insights:\n",
        "\n",
        "creation of a good search engine ist hard. The method described mostly links due to words like \"the\", \"or\", \"and\" etc. since they are frequent. A much larger database might reduce this issue, but the more promising approach is to exclude \"common\" words from the analysis. I.e. before removing them, the entry for \"Duisburg\" was often the best fit for my queries regarding classical music, since it shares many word of the \"is\" \"a\" \"of\" etc. type with the texts refering to classical music. Once they are removed, the more appropriate hits of Beethoven and Bonn are before Duisburg. Also when taking sequences of words into account, instead of just sequences inside words (as I do here), sequences of \"in the\" etc. pollute the anaysis.\n",
        "\n",
        "Also I find that, contrary to the example before, a k value of about 5 to 6 is useful in finding propper connections, when dealing with only the 8 original wikipedia texts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgmLfKp0KLl3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 14,
      "outputs": []
    }
  ]
}